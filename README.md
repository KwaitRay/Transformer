# Transformer
This project is designed to explore the attention mechanism and how to build an transformer model. The transformer model consists of encoder and decoder block, multi-attention ,self-attention, position-coding is included as well. By collecting the Eng-France dataset online, and through training, I completed a model can be used in text translation.
## 一.介绍
### 1.项目背景
### 2.技术栈
#### （1）
详细记录背景和假设：确保对项目的背景、目标和假设做详细清晰的说明，帮助他人理解你的工作。
清晰描述方法和过程：对于方法论和实验步骤，要描述清晰，确保别人能够根据你的描述理解每一个关键的步骤，包括算法原理、数据处理流程、模型设计等。
提供充分的实验验证：如果项目包含实验，确保数据集、实验设置、评估指标等都描述明确，并且提供必要的结果支持，避免模糊不清的结果或解释。
使用清晰的语言和格式：避免过于复杂的语言和专业术语，尽量使用简洁明了的表达，使得描述可以被广泛理解。适当时可以配合图表、公式等辅助说明。
## 二.模型细节
### 1.Transformer模型简要原理
### 2.模型实现
#### （1）多头注意力机制
#### （2）自注意力机制
#### （3）位置编码
#### （4）编码器
#### （5）解码器

